{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiSearch Agent RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun, ArxivQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "arxiv_api_wrapper = ArxivAPIWrapper()\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_api_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Store documents in vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/concepts/evaluation\")\n",
    "docs=loader.load()\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "vectordb=FAISS.from_documents(documents,OpenAIEmbeddings())\n",
    "retriever=vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Retrival Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrival_tool= create_retriever_tool(retriever,\"langsmith_search\",description=\"Search for information about LangSmith eval documentation. For any questions about LangSmith, you must use this tool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wikipedia,arxiv, retrival_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "agent=create_openai_tools_agent(llm,tools,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agent Executer\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith_search` with `{'query': 'create dataset'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mThis can be a useful way to get a lot of datapoints, quickly.tipTo learn more about creating datasets in LangSmith, see our LangSmith Evaluation series:See our video on Manually curated datasets.See our videos on Datasets from tracesTypes of datasetsâ€‹LangSmith offers three distinct dataset types:kv (Key-Value) Dataset:\"Inputs\" and \"outputs\" are represented as arbitrary key-value pairs.The kv dataset is the most versatile and default type, suitable for a wide range of evaluation scenarios.This dataset type is ideal for evaluating chains and agents that require multiple inputs or generate multiple outputs.llm (Language Model) Dataset:The llm dataset is designed for evaluating \"completion\" style language models.The \"inputs\" dictionary contains a single \"input\" key mapped to the prompt string.The \"outputs\" dictionary contains a single \"output\" key mapped to the corresponding response string.This dataset type simplifies evaluation for LLMs by providing a standardized format for inputs\n",
      "\n",
      "If possible - try to collect end user feedback. You can then see which datapoints got negative feedback.\n",
      "That is super valuable! These are spots where your application did not perform well.\n",
      "You should add these to your dataset to test against in the future. You can also use other heuristics\n",
      "to identify \"interesting\" datapoints - for example, runs that took a long time to complete could be interesting to look at and add to a dataset.Synthetic dataOnce you have a few examples, you can try to artificially generate examples.\n",
      "It's generally advised to have a few good hand-craft examples before this, as this synthetic data will often resemble them in some way.\n",
      "\n",
      "inputs to your application used for conducting evaluations.Evaluator: An evaluator is a function responsible for scoring your AI application based on the provided dataset.Datasetsâ€‹Datasets are the cornerstone of the LangSmith evaluation workflow. They are collections of examples that provide the necessary inputs and, optionally, expected reference outputs for assessing your AI application. Each example within a dataset represents a single data point, consisting of an inputs dictionary, an optional output dictionary, and an optional metadata dictionary. The optional output dictionary will often contain a reference key, which is the expected LLM application output for the given input.Creating datasetsâ€‹There are various ways to build datasets for evaluation, including:Manually curated examplesThis is how we typically recommend people get started creating datasets.\n",
      "\n",
      "This can be the case for tasks like summarization - it may be hard to give a summary a perfect score on a scale of 1-10, but easier to tell if it's better than a baseline.Applying evaluationsâ€‹We can visualize the above ideas collectively in the below diagram. To review, datasets are composed of examples that can be curated from a variety of sources such as historical logs or user curated examples. Evaluators are functions that score how well your application performs on each example in your dataset. Evaluators can use different scoring functions, such as human, heuristic, LLM-as-judge, or pairwise. And if the dataset contains reference outputs, then the evaluator can compare the application output to the reference.Each time we run an evaluation, we are conducting an experiment. An experiment is a single execution of all the example inputs in your dataset through your task. Typically, we will run multiple experiments on a given dataset, testing different tweaks to our task (e.g.,\u001b[0m\u001b[32;1m\u001b[1;3mTo create a dataset in LangSmith, you can follow these steps:\n",
      "\n",
      "1. Manually Curated Examples: Start by manually curating examples that represent the inputs and expected outputs for your AI application. This is a recommended way to begin creating datasets.\n",
      "\n",
      "2. Types of Datasets: LangSmith offers three distinct dataset types:\n",
      "   - Key-Value (kv) Dataset: Represents inputs and outputs as key-value pairs, suitable for a wide range of evaluation scenarios.\n",
      "   - Language Model (llm) Dataset: Designed for evaluating completion-style language models, with standardized input and output formats.\n",
      "   - Synthetic Data: After creating a few hand-crafted examples, you can generate synthetic data to expand your dataset.\n",
      "\n",
      "3. Collect End User Feedback: Gather feedback from end users to identify areas where your application may not perform well. Incorporate negative feedback into your dataset for future testing.\n",
      "\n",
      "4. Apply Evaluations: Use evaluators to score your AI application based on the dataset. Evaluators can use different scoring functions such as human evaluation, heuristic evaluation, LLM-as-judge, or pairwise comparison.\n",
      "\n",
      "5. Experimentation: Run multiple experiments on your dataset to test different tweaks to your task and evaluate the performance of your AI application.\n",
      "\n",
      "By following these steps, you can effectively create datasets in LangSmith for evaluating your AI application.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How to create dataset in langsmith',\n",
       " 'output': 'To create a dataset in LangSmith, you can follow these steps:\\n\\n1. Manually Curated Examples: Start by manually curating examples that represent the inputs and expected outputs for your AI application. This is a recommended way to begin creating datasets.\\n\\n2. Types of Datasets: LangSmith offers three distinct dataset types:\\n   - Key-Value (kv) Dataset: Represents inputs and outputs as key-value pairs, suitable for a wide range of evaluation scenarios.\\n   - Language Model (llm) Dataset: Designed for evaluating completion-style language models, with standardized input and output formats.\\n   - Synthetic Data: After creating a few hand-crafted examples, you can generate synthetic data to expand your dataset.\\n\\n3. Collect End User Feedback: Gather feedback from end users to identify areas where your application may not perform well. Incorporate negative feedback into your dataset for future testing.\\n\\n4. Apply Evaluations: Use evaluators to score your AI application based on the dataset. Evaluators can use different scoring functions such as human evaluation, heuristic evaluation, LLM-as-judge, or pairwise comparison.\\n\\n5. Experimentation: Run multiple experiments on your dataset to test different tweaks to your task and evaluate the performance of your AI application.\\n\\nBy following these steps, you can effectively create datasets in LangSmith for evaluating your AI application.'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"How to create dataset in langsmith\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Chief Minister of Uttar Pradesh'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: List of chief ministers of Uttar Pradesh\n",
      "Summary: The Chief Minister of Uttar Pradesh is the head of the Government of Uttar Pradesh. As per the Constitution of India, the governor is the state's de jure head, but de facto executive authority rests with the chief minister. Following elections to the Uttar Pradesh Legislative Assembly, the governor usually invites the party (or coalition) with a majority of seats to form the government. The governor appoints the chief minister, whose council of ministers are collectively responsible to the assembly. Given that he has the confidence of the assembly, the chief minister's term is for five years and is subject to no term limits.\n",
      "On 26 January 1950 Govind Ballabh Pant, premier of United Provinces, became the first chief minister of the newly renamed Uttar Pradesh. Including him, 11 out of UP's 21 chief ministers belonged to the Indian National Congress. Among these is V. P. Singh, a future prime minister of India, as was Charan Singh of the Bharatiya Lok Dal. On ten occasions, most recently in 2002, the state has come under President's rule, leaving the office of chief minister vacant. UP has also had two women chief ministers—Sucheta Kripalani and Mayawati. Akhilesh Yadav of the Samajwadi Party served as the chief minister of Uttar Pradesh from 2012 to 2017; having taken the oath at 38 years of age, he is the youngest person to have held the office. Only three chief ministers completed their official tenure of five years: Mayawati, Akhilesh Yadav, and Yogi Adityanath.\n",
      "Yogi Adityanath of the Bharatiya Janata Party is serving as the incumbent chief minister since 19 March 2017.\n",
      "\n",
      "Page: List of deputy chief ministers of Uttar Pradesh\n",
      "Summary: The Deputy Chief Minister of Uttar Pradesh is a member of the Cabinet of Uttar Pradesh Government in the Government of Uttar Pradesh. Not a constitutional office, it seldom carries any specific powers. A deputy chief minister usually also holds a cabinet portfolio such as home minister or finance minister. In the parliamentary system of government, the Chief Minister is treated as the \"first among equals\" in the cabinet; the position of deputy chief minister is used to bring political stability and strength within a coalition government. \n",
      "The post of Deputy Chief Minister is a kind of informal post, which is not mentioned in the Constitution. It works like an ordinary cabinet minister. The appointment of Deputy Chief Minister is done by the ruling party only to maintain balance in the political situation. There is no fixed number for the post of Deputy Chief Minister, in any state it can be made two, three or even more. For example, in 1998, Madhya Pradesh had two deputy chief ministers, Subhash Yadav and Jamuna Devi, under Chief Minister Digvijay Singh. Similarly, after the 2017 Uttar Pradesh assembly elections, two Deputy Chief Ministers Dinesh Sharma  and Keshav Prasad Maurya were appointed under Chief Minister Yogi Adityanath to correct the electoral equations. Babu Narain Singh Gurjar was first Deputy Chief Minister of Uttar Pradesh.\n",
      "\n",
      "Page: Government of Uttar Pradesh\n",
      "Summary: The Government of Uttar Pradesh (ISO: Uttara Pradēśa Sarakāra; often abbreviated as GoUP) is the subnational government of the Indian state of Uttar Pradesh with the governor as its appointed constitutional head of the state by the President of India. The Governor of Uttar Pradesh is appointed for a period of five years and appoints the Chief Minister of Uttar Pradesh and their council of ministers, who are vested with the executive powers of the state. The governor remains a ceremonial head of the state, while the chief minister and their council are responsible for day-to-day government functions. \n",
      "The state of Uttar Pradesh's influence on Indian politics is important, and often paramount and/or a bellwether, as it sends the most members of parliament to both the Lok Sabha and the Rajya Sabha, the state's population being more than 200 million; approximately dou\u001b[0m\u001b[32;1m\u001b[1;3mYogi Adityanath of the Bharatiya Janata Party is currently serving as the Chief Minister of Uttar Pradesh since 19 March 2017.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who is CM of UP',\n",
       " 'output': 'Yogi Adityanath of the Bharatiya Janata Party is currently serving as the Chief Minister of Uttar Pradesh since 19 March 2017.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Who is CM of UP\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': 'The Matrix Calculus You Need For Deep Learning'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2018-07-02\n",
      "Title: The Matrix Calculus You Need For Deep Learning\n",
      "Authors: Terence Parr, Jeremy Howard\n",
      "Summary: This paper is an attempt to explain all the matrix calculus you need in order\n",
      "to understand the training of deep neural networks. We assume no math knowledge\n",
      "beyond what you learned in calculus 1, and provide links to help you refresh\n",
      "the necessary math where needed. Note that you do not need to understand this\n",
      "material before you start learning to train and use deep learning in practice;\n",
      "rather, this material is for those who are already familiar with the basics of\n",
      "neural networks, and wish to deepen their understanding of the underlying math.\n",
      "Don't worry if you get stuck at some point along the way---just go back and\n",
      "reread the previous section, and try writing down and working through some\n",
      "examples. And if you're still stuck, we're happy to answer your questions in\n",
      "the Theory category at forums.fast.ai. Note: There is a reference section at\n",
      "the end of the paper summarizing all the key matrix calculus rules and\n",
      "terminology discussed here. See related articles at http://explained.ai\n",
      "\n",
      "Published: 2023-12-01\n",
      "Title: How To Program Your Own Quantum Computer or QUBE: QUantum computing for BEginners\n",
      "Authors: Martin N. P. Nilsson\n",
      "Summary: Do you think you need to know quantum physics to understand how a quantum\n",
      "computer works? Nope, no worries there. You don't need a deep dive into physics\n",
      "or mathematics, just a bit of familiarity with vectors and matrix\n",
      "multiplication. That's really it. A good handle on Python programming and a few\n",
      "numpy functions will do the trick, specifically reshape(), kron(), matmul(),\n",
      "swapaxes(), linalg.norm(), and random.choice(). In fact, an appendix shows that\n",
      "twelve lines of Python code suffice to define a complete simulator.\n",
      "  The whole point of this article is to give you an informal, brief, hopefully\n",
      "digestible and educational description of how you can easily implement your own\n",
      "quantum computer simulator. It's not about `Yet Another Quantum Computer\n",
      "Simulator' (YAQCS?), which are a dime a dozen, but about how to build your own.\n",
      "And, honestly, there's probably no better way to learn how a quantum computer\n",
      "works!\n",
      "\n",
      "Published: 2020-06-16\n",
      "Title: Robust Recovery via Implicit Bias of Discrepant Learning Rates for Double Over-parameterization\n",
      "Authors: Chong You, Zhihui Zhu, Qing Qu, Yi Ma\n",
      "Summary: Recent advances have shown that implicit bias of gradient descent on\n",
      "over-parameterized models enables the recovery of low-rank matrices from linear\n",
      "measurements, even with no prior knowledge on the intrinsic rank. In contrast,\n",
      "for robust low-rank matrix recovery from grossly corrupted measurements,\n",
      "over-parameterization leads to overfitting without prior knowledge on both the\n",
      "intrinsic rank and sparsity of corruption. This paper shows that with a double\n",
      "over-parameterization for both the low-rank matrix and sparse corruption,\n",
      "gradient descent with discrepant learning rates provably recovers the\n",
      "underlying matrix even without prior knowledge on neither rank of the matrix\n",
      "nor sparsity of the corruption. We further extend our approach for the robust\n",
      "recovery of natural images by over-parameterizing images with deep\n",
      "convolutional networks. Experiments show that our method handles different test\n",
      "images and varying corruption levels with a single learning pipeline where the\n",
      "network width and termination conditions do not need to be adjusted on a\n",
      "case-by-case basis. Underlying the success is again the implicit bias with\n",
      "discrepant learning rates on different over-parameterized parameters, which may\n",
      "bear on broader applications.\u001b[0m\u001b[32;1m\u001b[1;3mThe paper titled \"The Matrix Calculus You Need For Deep Learning\" by Terence Parr and Jeremy Howard provides an explanation of all the matrix calculus necessary to understand the training of deep neural networks. It assumes no math knowledge beyond what you learned in calculus 1 and offers links to refresh the necessary math concepts. This material is intended for those already familiar with the basics of neural networks who wish to deepen their understanding of the underlying math. If you have any questions or need further clarification, you can seek help in the Theory category at forums.fast.ai.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'The Matrix Calculus You Need For Deep Learning',\n",
       " 'output': 'The paper titled \"The Matrix Calculus You Need For Deep Learning\" by Terence Parr and Jeremy Howard provides an explanation of all the matrix calculus necessary to understand the training of deep neural networks. It assumes no math knowledge beyond what you learned in calculus 1 and offers links to refresh the necessary math concepts. This material is intended for those already familiar with the basics of neural networks who wish to deepen their understanding of the underlying math. If you have any questions or need further clarification, you can seek help in the Theory category at forums.fast.ai.'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"The Matrix Calculus You Need For Deep Learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
